<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>凸优化</title>
      <link href="/2018/12/12/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
      <url>/2018/12/12/%E5%87%B8%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h2 id="对凸优化局部最优即为全局最优的证明"><a href="#对凸优化局部最优即为全局最优的证明" class="headerlink" title="对凸优化局部最优即为全局最优的证明"></a>对凸优化局部最优即为全局最优的证明</h2><p>转自:<a href="http://sofasofa.io/forum_main_post.php?postid=1000322&amp;" target="_blank" rel="noopener">链接</a></p><p>用反证法来证明。已知$x_0$是个局部最优点，假设在全集$S$上存在一点$x_*$,使得</p><script type="math/tex; mode=display">f(x_*) < f(x_0).</script><p>因为$f(x)$是凸函数，所以对于任意的$t$</p><script type="math/tex; mode=display">f(tx_*+(1-t)x_0)\leq tf(x_*) + (1-t)f(x_0)</script><p>注意，这个$t$是0到1之间的任意值，所以$t$可以非常接近0，此时$(tx_*+(1−t)x_0)$这个点就可以无限接近$x_0$，但是函数在这个点的值又比$f(x_0)$小，所以$f(x_0)$不可能是局部最小值。故假设矛盾，因此不存在这样的$x_∗$。$f(x_0)$必定为最小值。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 凸优化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>tf.layers.Dense的内部运算</title>
      <link href="/2018/12/12/tf-layers-Dense%E7%9A%84%E5%86%85%E9%83%A8%E8%BF%90%E7%AE%97/"/>
      <url>/2018/12/12/tf-layers-Dense%E7%9A%84%E5%86%85%E9%83%A8%E8%BF%90%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<p>tf.layers.Dense函数实现了全连接层，有一个值得关注的问题是在接收的输入维度是[batchsize,m,n]这样的三维张量时，函数是如何计算的。<br>测试代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.enable_eager_execution()</span><br><span class="line">input = np.ones([<span class="number">4</span>,<span class="number">3</span>,<span class="number">6</span>])</span><br><span class="line">input[<span class="number">0</span>,<span class="number">1</span>]=[<span class="number">2.</span>,<span class="number">2.</span>,<span class="number">2.</span>,<span class="number">2.</span>,<span class="number">2.</span>,<span class="number">2.</span>]</span><br><span class="line">input=tf.constant(input)</span><br><span class="line">print(input)</span><br><span class="line">u=tf.layers.Dense(<span class="number">10</span>) <span class="comment">##10为out_units</span></span><br><span class="line">y=u(input)</span><br><span class="line">print(np.sum(u.get_weights()[<span class="number">0</span>],<span class="number">0</span>))</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure></p><p>输出：<br>tf.Tensor(<br>[[[1. 1. 1. 1. 1. 1.]<br>  [2. 2. 2. 2. 2. 2.]<br>  [1. 1. 1. 1. 1. 1.]]</p><p> [[1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]]</p><p> [[1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]]</p><p> [[1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]]], shape=(4, 3, 6), dtype=float64)<br>[-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638  0.28534595<br> -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>tf.Tensor(<br>[[[-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-1.98283793 -0.99260048 -1.8802868  -1.79091705 -4.47063277<br>    0.5706919  -0.03517144 -1.2777074  -1.99043485 -1.18590682]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]]</p><p> [[-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]]</p><p> [[-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]]</p><p> [[-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]]], shape=(4, 3, 10), dtype=float64)</p><p>可以发现，权重矩阵W的shape为[6,10]，即[n,out_units]，结果也表明，运算是把batch中的每个[m,n]当做m个n维向量分别进行运算的，输出结果是m个out_units维的向量。</p>]]></content>
      
      
      
        <tags>
            
            <tag> tf函数 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>书签</title>
      <link href="/2018/12/11/%E4%B9%A6%E7%AD%BE/"/>
      <url>/2018/12/11/%E4%B9%A6%E7%AD%BE/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/lovychen/p/9368390.html" target="_blank" rel="noopener">RNN、Lstm源码，训练代码输入输出，维度分析</a><br><a href="https://www.cnblogs.com/loveyouyou616/p/7525863.html" target="_blank" rel="noopener">python调试工具pdb</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&amp;mid=2247485192&amp;idx=1&amp;sn=4320fd2a6e94aef7cba1a6c7a68f3b26&amp;chksm=fb39a203cc4e2b155d58611ff3db8ddc93764e62e1ceae7408a090eb3f47930c1888d48afcf1&amp;mpshare=1&amp;scene=23&amp;srcid=1212j2nZKtddupp89McBeF7K#rd" target="_blank" rel="noopener">GBDT</a><br><a href="https://blog.csdn.net/u014540876/article/details/79153913" target="_blank" rel="noopener">为什么拉格朗日对偶函数一定是凹函数(逐点下确界)</a></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>han数据预处理代码注释</title>
      <link href="/2018/12/10/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/"/>
      <url>/2018/12/10/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<h3 id="源码：源码地址"><a href="#源码：源码地址" class="headerlink" title="源码：源码地址"></a>源码：<a href="https://github.com/triplemeng/hierarchical-attention-model" target="_blank" rel="noopener">源码地址</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_review</span><span class="params">(data, sent_length, max_rev_len, keep_in_dict=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    <span class="comment">## As the result, each review will be composed of max_rev_len sentences. If the original review is longer than that, we truncate it, and if shorter than that, we append empty sentences to it. And each sentence will be composed of sent_length words. If the original sentence is longer than that, we truncate it, and if shorter, we append the word of 'UNK' to it. Also, we keep track of the actual number of sentences each review contains.</span></span><br><span class="line">    data_formatted = []</span><br><span class="line">    review_lens = []</span><br><span class="line">    <span class="keyword">for</span> review <span class="keyword">in</span> data:</span><br><span class="line">        review_formatted = preprocessing.sequence.pad_sequences(review, maxlen=sent_length, padding=<span class="string">"post"</span>, truncating=<span class="string">"post"</span>, value=keep_in_dict+<span class="number">1</span>)</span><br><span class="line">        review_len = review_formatted.shape[<span class="number">0</span>]</span><br><span class="line">        review_lens.append(review_len <span class="keyword">if</span> review_len&lt;=max_rev_len <span class="keyword">else</span> max_rev_len)</span><br><span class="line">        lack_len = max_rev_length - review_len</span><br><span class="line">        review_formatted_right_len = review_formatted</span><br><span class="line">        <span class="keyword">if</span> lack_len &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment">#extra_rows = np.zeros([lack_len, sent_length], dtype=np.int32)</span></span><br><span class="line">            extra_rows = np.full((lack_len, sent_length), keep_in_dict+<span class="number">1</span>)</span><br><span class="line">            review_formatted_right_len = np.append(review_formatted, extra_rows, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> lack_len &lt; <span class="number">0</span>:</span><br><span class="line">            row_index = [max_rev_length+i <span class="keyword">for</span> i <span class="keyword">in</span> list(range(<span class="number">0</span>, -lack_len))]</span><br><span class="line">            review_formatted_right_len = np.delete(review_formatted, row_index, axis=<span class="number">0</span>)</span><br><span class="line">        data_formatted.append(review_formatted_right_len)</span><br><span class="line">    <span class="keyword">return</span> data_formatted, review_lens</span><br></pre></td></tr></table></figure><h3 id="preprocessing-sequence-pad-sequences-sequences-maxlen-None-dtype-’int32’-padding-’pre’-truncating-’pre’-value-0-函数："><a href="#preprocessing-sequence-pad-sequences-sequences-maxlen-None-dtype-’int32’-padding-’pre’-truncating-’pre’-value-0-函数：" class="headerlink" title="preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.) 函数："></a>preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.) 函数：</h3><h4 id="参数："><a href="#参数：" class="headerlink" title="参数："></a>参数：</h4><p>sequences：浮点数或整数构成的两层嵌套列表</p><p>maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.</p><p>dtype：返回的numpy array的数据类型</p><p>padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补</p><p>truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断</p><p>value：浮点数，此值将在填充时代替默认的填充值0</p><p>返回值<br>返回形如(nb_samples,nb_timesteps)的2D张量</p><p>例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">review=[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]</span><br><span class="line">review_formatted = preprocessing.sequence.pad_sequences(review, maxlen=<span class="number">5</span>, padding=<span class="string">"post"</span>, truncating=<span class="string">"post"</span>, value=<span class="number">5</span>+<span class="number">1</span>)</span><br><span class="line">print(review_formatted)</span><br></pre></td></tr></table></figure></p><p>输出：<br>[[1 2 3 6 6]<br> [1 2 3 4 6]<br> [1 1 1 1 1]]</p><h3 id="np-full-和-np-delete"><a href="#np-full-和-np-delete" class="headerlink" title="np.full() 和 np.delete() :"></a>np.full() 和 np.delete() :</h3><h4 id="np-full-shape-fill-value-dtype-None-order-’C’"><a href="#np-full-shape-fill-value-dtype-None-order-’C’" class="headerlink" title="np.full(shape, fill_value, dtype=None, order=’C’)"></a>np.full(shape, fill_value, dtype=None, order=’C’)</h4><p>Return a new array of given shape and type, filled with fill_value.</p><h4 id="numpy-delete-arr-obj-axis-None"><a href="#numpy-delete-arr-obj-axis-None" class="headerlink" title="numpy.delete(arr, obj, axis=None)"></a>numpy.delete(arr, obj, axis=None)</h4><p>Return a new array with sub-arrays along an axis deleted. For a one dimensional array, this returns those entries not returned by arr[obj].<br>例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">review=np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">                 [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line">                 [<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">print(np.delete(review,[<span class="number">0</span>,<span class="number">1</span>],<span class="number">0</span>))</span><br><span class="line">print(np.delete(review,[<span class="number">0</span>,<span class="number">1</span>],<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>输出：<br>[[ 9 10 11 12]]<br>[[ 3  4]<br> [ 7  8]<br> [11 12]]</p>]]></content>
      
      
      
        <tags>
            
            <tag> han preprocessing </tag>
            
            <tag> np.full </tag>
            
            <tag> np.delete </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>My New Post</title>
      <link href="/2018/12/06/My-New-Post/"/>
      <url>/2018/12/06/My-New-Post/</url>
      
        <content type="html"><![CDATA[<p>A little bit of work, very ashamed。口—口</p>]]></content>
      
      
      
    </entry>
    
  
  
</search>
