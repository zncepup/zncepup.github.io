<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>正则表达式</title>
      <link href="/2019/01/09/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2019/01/09/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding: utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义获取文本电子邮件的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_findAll_emails</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param text: 文本</span></span><br><span class="line"><span class="string">    :return: 返回电子邮件列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    emails = re.findall(<span class="string">r"[a-z0-9\.\-+_]+@[a-z0-9\.\-+_]+\.[a-z]+"</span>, text)</span><br><span class="line">    <span class="keyword">return</span> emails</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义获取文本手机号函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_findAll_mobiles</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param text: 文本</span></span><br><span class="line"><span class="string">    :return: 返回手机号列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mobiles = re.findall(<span class="string">r"1\d&#123;10&#125;"</span>, text)</span><br><span class="line">    <span class="keyword">return</span> mobiles</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义获取文本url函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_findAll_urls</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param text: 文本</span></span><br><span class="line"><span class="string">    :return: 返回url列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    urls=re.findall(<span class="string">r"(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)|([a-zA-Z]+.\w+\.+[a-zA-Z0-9\/_]+)"</span>,text)</span><br><span class="line">    urls=list(sum(urls,()))</span><br><span class="line">urls=[x <span class="keyword">for</span> x <span class="keyword">in</span> urls <span class="keyword">if</span> x!=<span class="string">''</span>]</span><br><span class="line">    <span class="keyword">return</span> urls</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义获取获取ip地址函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_findAll_ips</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param text: 文本</span></span><br><span class="line"><span class="string">    :return: 返回ip列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ips = re.findall(<span class="string">r"\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.)&#123;3&#125;(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b"</span>, text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ips</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    </span><br><span class="line">    content = <span class="string">"Please 42.121.252.58:443 contact 127.0.0.1  15988455173 us 18720071239 https://blog.csdn.net/u013421629/ at https://www.yiibai.com/ contact@qq.com for further information 1973536419@qq.com You can  also give feedbacl at feedback@yiibai.com"</span></span><br><span class="line">    emails=get_findAll_emails(text=content)</span><br><span class="line">    <span class="keyword">print</span> emails</span><br><span class="line">    moblies=get_findAll_mobiles(text=content)</span><br><span class="line">    <span class="keyword">print</span> moblies</span><br><span class="line">    urls=get_findAll_urls(text=content)</span><br><span class="line">    <span class="keyword">print</span> urls</span><br><span class="line">    ips=get_findAll_ips(text=content)</span><br><span class="line">    <span class="keyword">print</span> ips</span><br></pre></td></tr></table></figure><p>原文：<a href="https://blog.csdn.net/u013421629/article/details/82918060" target="_blank" rel="noopener">https://blog.csdn.net/u013421629/article/details/82918060</a> </p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>Understanding the difficulty of training deep feedforward neural networks</title>
      <link href="/2019/01/07/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks/"/>
      <url>/2019/01/07/Understanding-the-difficulty-of-training-deep-feedforward-neural-networks/</url>
      
        <content type="html"><![CDATA[<h2 id="xavierc初始化"><a href="#xavierc初始化" class="headerlink" title="xavierc初始化"></a>xavierc初始化</h2>]]></content>
      
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>最大熵模型</title>
      <link href="/2018/12/20/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/12/20/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>熵衡量信息量，与不确定性成正相关。最大熵模型中的熵指的是条件熵：</p><script type="math/tex; mode=display">H(Y|X)</script>]]></content>
      
      
      
        <tags>
            
            <tag> 熵 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>凸优化</title>
      <link href="/2018/12/12/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
      <url>/2018/12/12/%E5%87%B8%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h2 id="对凸优化局部最优即为全局最优的证明"><a href="#对凸优化局部最优即为全局最优的证明" class="headerlink" title="对凸优化局部最优即为全局最优的证明"></a>对凸优化局部最优即为全局最优的证明</h2><p>转自:<a href="http://sofasofa.io/forum_main_post.php?postid=1000322&amp;" target="_blank" rel="noopener">链接</a></p><p>用反证法来证明。已知$x_0$是个局部最优点，假设在全集$S$上存在一点$x_*$,使得</p><script type="math/tex; mode=display">f(x_*) < f(x_0).</script><p>因为$f(x)$是凸函数，所以对于任意的$t$</p><script type="math/tex; mode=display">f(tx_*+(1-t)x_0)\leq tf(x_*) + (1-t)f(x_0)</script><p>注意，这个$t$是0到1之间的任意值，所以$t$可以非常接近0，此时$(tx_*+(1−t)x_0)$这个点就可以无限接近$x_0$，但是函数在这个点的值又比$f(x_0)$小，所以$f(x_0)$不可能是局部最小值。故假设矛盾，因此不存在这样的$x_∗$。$f(x_0)$必定为最小值。</p><h2 id="仿射集和凸集转"><a href="#仿射集和凸集转" class="headerlink" title="仿射集和凸集转"></a>仿射集和凸集<a href="https://blog.csdn.net/weixin_35732969/article/details/82561745" target="_blank" rel="noopener">转</a></h2>]]></content>
      
      
      
        <tags>
            
            <tag> 凸优化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>tf.layers.Dense的内部运算</title>
      <link href="/2018/12/12/tf-layers-Dense%E7%9A%84%E5%86%85%E9%83%A8%E8%BF%90%E7%AE%97/"/>
      <url>/2018/12/12/tf-layers-Dense%E7%9A%84%E5%86%85%E9%83%A8%E8%BF%90%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<p>tf.layers.Dense函数实现了全连接层，有一个值得关注的问题是在接收的输入维度是[batchsize,m,n]这样的三维张量时，函数是如何计算的。<br>测试代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.enable_eager_execution()</span><br><span class="line">input = np.ones([<span class="number">4</span>,<span class="number">3</span>,<span class="number">6</span>])</span><br><span class="line">input[<span class="number">0</span>,<span class="number">1</span>]=[<span class="number">2.</span>,<span class="number">2.</span>,<span class="number">2.</span>,<span class="number">2.</span>,<span class="number">2.</span>,<span class="number">2.</span>]</span><br><span class="line">input=tf.constant(input)</span><br><span class="line">print(input)</span><br><span class="line">u=tf.layers.Dense(<span class="number">10</span>) <span class="comment">##10为out_units</span></span><br><span class="line">y=u(input)</span><br><span class="line">print(np.sum(u.get_weights()[<span class="number">0</span>],<span class="number">0</span>))</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure></p><p>输出：<br>tf.Tensor(<br>[[[1. 1. 1. 1. 1. 1.]<br>  [2. 2. 2. 2. 2. 2.]<br>  [1. 1. 1. 1. 1. 1.]]</p><p> [[1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]]</p><p> [[1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]]</p><p> [[1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]<br>  [1. 1. 1. 1. 1. 1.]]], shape=(4, 3, 6), dtype=float64)<br>[-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638  0.28534595<br> -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>tf.Tensor(<br>[[[-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-1.98283793 -0.99260048 -1.8802868  -1.79091705 -4.47063277<br>    0.5706919  -0.03517144 -1.2777074  -1.99043485 -1.18590682]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]]</p><p> [[-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]]</p><p> [[-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]]</p><p> [[-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]<br>  [-0.99141896 -0.49630024 -0.9401434  -0.89545853 -2.23531638<br>    0.28534595 -0.01758572 -0.6388537  -0.99521742 -0.59295341]]], shape=(4, 3, 10), dtype=float64)</p><p>可以发现，权重矩阵W的shape为[6,10]，即[n,out_units]，结果也表明，运算是把batch中的每个[m,n]当做m个n维向量分别进行运算的，输出结果是m个out_units维的向量。</p>]]></content>
      
      
      
        <tags>
            
            <tag> tf函数 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>书签</title>
      <link href="/2018/12/11/%E4%B9%A6%E7%AD%BE/"/>
      <url>/2018/12/11/%E4%B9%A6%E7%AD%BE/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/lovychen/p/9368390.html" target="_blank" rel="noopener">RNN、Lstm源码，训练代码输入输出，维度分析</a><br><a href="https://www.cnblogs.com/loveyouyou616/p/7525863.html" target="_blank" rel="noopener">python调试工具pdb</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&amp;mid=2247485192&amp;idx=1&amp;sn=4320fd2a6e94aef7cba1a6c7a68f3b26&amp;chksm=fb39a203cc4e2b155d58611ff3db8ddc93764e62e1ceae7408a090eb3f47930c1888d48afcf1&amp;mpshare=1&amp;scene=23&amp;srcid=1212j2nZKtddupp89McBeF7K#rd" target="_blank" rel="noopener">GBDT</a><br><a href="https://blog.csdn.net/u014540876/article/details/79153913" target="_blank" rel="noopener">为什么拉格朗日对偶函数一定是凹函数(逐点下确界)</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 书签 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>han数据预处理代码注释</title>
      <link href="/2018/12/10/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/"/>
      <url>/2018/12/10/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<h3 id="源码：源码地址"><a href="#源码：源码地址" class="headerlink" title="源码：源码地址"></a>源码：<a href="https://github.com/triplemeng/hierarchical-attention-model" target="_blank" rel="noopener">源码地址</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_review</span><span class="params">(data, sent_length, max_rev_len, keep_in_dict=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    <span class="comment">## As the result, each review will be composed of max_rev_len sentences. If the original review is longer than that, we truncate it, and if shorter than that, we append empty sentences to it. And each sentence will be composed of sent_length words. If the original sentence is longer than that, we truncate it, and if shorter, we append the word of 'UNK' to it. Also, we keep track of the actual number of sentences each review contains.</span></span><br><span class="line">    data_formatted = []</span><br><span class="line">    review_lens = []</span><br><span class="line">    <span class="keyword">for</span> review <span class="keyword">in</span> data:</span><br><span class="line">        review_formatted = preprocessing.sequence.pad_sequences(review, maxlen=sent_length, padding=<span class="string">"post"</span>, truncating=<span class="string">"post"</span>, value=keep_in_dict+<span class="number">1</span>)</span><br><span class="line">        review_len = review_formatted.shape[<span class="number">0</span>]</span><br><span class="line">        review_lens.append(review_len <span class="keyword">if</span> review_len&lt;=max_rev_len <span class="keyword">else</span> max_rev_len)</span><br><span class="line">        lack_len = max_rev_length - review_len</span><br><span class="line">        review_formatted_right_len = review_formatted</span><br><span class="line">        <span class="keyword">if</span> lack_len &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment">#extra_rows = np.zeros([lack_len, sent_length], dtype=np.int32)</span></span><br><span class="line">            extra_rows = np.full((lack_len, sent_length), keep_in_dict+<span class="number">1</span>)</span><br><span class="line">            review_formatted_right_len = np.append(review_formatted, extra_rows, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> lack_len &lt; <span class="number">0</span>:</span><br><span class="line">            row_index = [max_rev_length+i <span class="keyword">for</span> i <span class="keyword">in</span> list(range(<span class="number">0</span>, -lack_len))]</span><br><span class="line">            review_formatted_right_len = np.delete(review_formatted, row_index, axis=<span class="number">0</span>)</span><br><span class="line">        data_formatted.append(review_formatted_right_len)</span><br><span class="line">    <span class="keyword">return</span> data_formatted, review_lens</span><br></pre></td></tr></table></figure><h3 id="preprocessing-sequence-pad-sequences-sequences-maxlen-None-dtype-’int32’-padding-’pre’-truncating-’pre’-value-0-函数："><a href="#preprocessing-sequence-pad-sequences-sequences-maxlen-None-dtype-’int32’-padding-’pre’-truncating-’pre’-value-0-函数：" class="headerlink" title="preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.) 函数："></a>preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.) 函数：</h3><h4 id="参数："><a href="#参数：" class="headerlink" title="参数："></a>参数：</h4><p>sequences：浮点数或整数构成的两层嵌套列表</p><p>maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.</p><p>dtype：返回的numpy array的数据类型</p><p>padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补</p><p>truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断</p><p>value：浮点数，此值将在填充时代替默认的填充值0</p><p>返回值<br>返回形如(nb_samples,nb_timesteps)的2D张量</p><p>例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">review=[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]</span><br><span class="line">review_formatted = preprocessing.sequence.pad_sequences(review, maxlen=<span class="number">5</span>, padding=<span class="string">"post"</span>, truncating=<span class="string">"post"</span>, value=<span class="number">5</span>+<span class="number">1</span>)</span><br><span class="line">print(review_formatted)</span><br></pre></td></tr></table></figure></p><p>输出：<br>[[1 2 3 6 6]<br> [1 2 3 4 6]<br> [1 1 1 1 1]]</p><h3 id="np-full-和-np-delete"><a href="#np-full-和-np-delete" class="headerlink" title="np.full() 和 np.delete() :"></a>np.full() 和 np.delete() :</h3><h4 id="np-full-shape-fill-value-dtype-None-order-’C’"><a href="#np-full-shape-fill-value-dtype-None-order-’C’" class="headerlink" title="np.full(shape, fill_value, dtype=None, order=’C’)"></a>np.full(shape, fill_value, dtype=None, order=’C’)</h4><p>Return a new array of given shape and type, filled with fill_value.</p><h4 id="numpy-delete-arr-obj-axis-None"><a href="#numpy-delete-arr-obj-axis-None" class="headerlink" title="numpy.delete(arr, obj, axis=None)"></a>numpy.delete(arr, obj, axis=None)</h4><p>Return a new array with sub-arrays along an axis deleted. For a one dimensional array, this returns those entries not returned by arr[obj].<br>例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">review=np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">                 [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line">                 [<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">print(np.delete(review,[<span class="number">0</span>,<span class="number">1</span>],<span class="number">0</span>))</span><br><span class="line">print(np.delete(review,[<span class="number">0</span>,<span class="number">1</span>],<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>输出：<br>[[ 9 10 11 12]]<br>[[ 3  4]<br> [ 7  8]<br> [11 12]]</p>]]></content>
      
      
      
        <tags>
            
            <tag> han preprocessing </tag>
            
            <tag> np.full </tag>
            
            <tag> np.delete </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>My New Post</title>
      <link href="/2018/12/06/My-New-Post/"/>
      <url>/2018/12/06/My-New-Post/</url>
      
        <content type="html"><![CDATA[<p>A little bit of work, very ashamed。口—口</p>]]></content>
      
      
      
    </entry>
    
  
  
</search>
